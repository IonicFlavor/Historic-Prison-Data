CN: 1) Visualization description: I visualized word frequencies in moral instructor’s notes of inmates and created a “Corpus A” with the characteristics Male, Illiterate, All Sobriety Categories, and All Admission Books and “Corpus B” with the characteristics Male, Reads and Writes, All Sobriety Categories, and All Admission Books. I wanted to isolate the literacy categorization and how it influenced the most common words in moral instructor’s notes. In the Comparative Frequencies plot, words such as “affirms”, “god”, “dead” and “parents” appear with near equal frequency in each plot. However, words associated with literate prisoners are generally more positive (“real”, “baptist”, “pious”, “business”, “child”, etc.) while words associated with illiterate prisoners are more negative (“enmity”, “stupid”, “ignorant”, etc.).
2) Relationship between visualization and repo: The word cloud processes all three admissions books in the repo (A, B, and D). It appears to look at the ColumnNote column and pulls out relevant words to determine whether an inmate can read, write, both, or neither. There is also an option for "Reads other language". The other levers that can be changed are "Admission Book" (A, B, D); "Sobriety" (Sober, Drinks, Drinks a Little, Drinks Often); and "Gender" (Female, Male, All). The visualization selects specific inmate entries based on the specifications and then plots commonly-repeated words from moral instructors' notes. A lot of these words, surprisingly, appear to not be articles such as "the", but more unique words such as "baptist" or "real".
3) Dara errors/limitations: The classification of "can read and write" (calling literate for ease) vs illiterate may be a cause for some confusion in the data since it appears it is based off the column called ColumnNote, which doesn’t clearly separate people who are slightly familiar with reading and writing versus people who are very fluent in reading and writing. Also, while some of these words in the Comparative Frequencies may initially appear positive, they don’t capture context (for example, one of the literate inmates was described as “professing to be God”, which is not positive). This means the connotations of these words can't necessarily be assumed. 
4) Additional visualizations + necessary data: It would be interesting to conduct a sentiment analysis on these words and see how they relate on performance assessment of prisoners (i.e. extending sentence based on bad conduct and reducing sentence based on good conduct) and looking at whether there is correlation with these description. For example, the number of sentence extensions / reductions could be plotted in relation to the positivity/negativity of the words moral instructors used to describe inmates. However, this would require additional data/information on the connotation surrounding these common words used to describe the inmates (i.e., the God example above); this might be difficult to conduct textual analysis on but there could be the incorporation of machine learning or human researchers marking whether or not certain words actually have positive or negative connotations. 

CN: 1) I performed an Eastern Apps data visualization on length of inmates’ sentence in months for ages 11-17 (what would be considered “minors” in current terms), then compared sentencing to ages 18-75. I kept all other variables the default (as “all”) to isolate the effects of age on inmate sentence. In general, I found that the max sentence length for two inmates <18 years of age was around 120 months and the minimum looked to be around 5 months, while the maximum sentence for inmates 18+ years old was ~240 months while the minimum was around a few months (difficult to tell from the graph). The distribution of sentence lengths for both age categories are right-skewed. Generally, there are a lot fewer recorded prisoners <18 years old. 
2) The repo data records information from admission books about inmates, and while some of the characterizing information is supposed to be taken in an objective lens (i.e., race, literacy), other pieces of information may be intended to inform recommended prison sentence length. The data records the prisoner’s age, as well as time in and time out. This particular visualization plots the prisoners’ sentences in months by number of prisoners sentenced and the data is segmented by age. The number of months is most likely surmised from the dates in and out.
3) Scanning through the raw data, there are some missing inputs in entries such as age and offense (but not many). This could impact the data, but it doesn’t appear that too many of these entries are missing (so likely not too large of an impact). Also, the age ranges provided on Eastern Apps range from 11-75 years old, making me wonder if there are any older inmates who are being excluded.
4) There are a few reasons why the data might be appearing like this: maybe there was more favorable sentencing of minors (even though they committed the same types of crimes that 18+ year olds committed), or these minors generally committed less severe crimes than adults. After looking through a bit of the raw data, I found some entries with serious offenses (i.e., Eliza Connelly or Conway, 16 years old, sentenced for poisoning); therefore, it might be more likely that there is more favorable sentencing of minors. It would be interesting to combine these data visualizations with qualitative research about sentencing legislation from the court to see if there were actually more favorable terms for younger prisoners/minors. Another interesting data visualization would be plotting the words from the crimes for <18 year old prisoners versus words from crimes for 18+ prisoners and seeing if there's any repeats of words, common words, or perhaps more severe crimes generally being performed by a specific segment of prisoners. This would require the data on type of crime committed, which is available in the Admission Books.